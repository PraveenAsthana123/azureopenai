apiVersion: v1
kind: ConfigMap
metadata:
  name: mlc-llm-config
  namespace: mlc-llm
data:
  serve-config.yaml: |
    server:
      host: "0.0.0.0"
      port: 8080
      enable_debug: false
      enable_metrics: true
      metrics_port: 9090

    models:
      - name: llama-3-1-70b
        model_id: meta-llama/Llama-3.1-70B-Instruct
        quantization: q4f16_1
        max_batch_size: 32
        tensor_parallel_shards: 4

      - name: llama-3-1-8b
        model_id: meta-llama/Llama-3.1-8B-Instruct
        quantization: q4f16_1
        max_batch_size: 64
        tensor_parallel_shards: 1

      - name: codellama-34b
        model_id: codellama/CodeLlama-34b-Instruct-hf
        quantization: q4f16_1
        max_batch_size: 32
        tensor_parallel_shards: 2

      - name: mistral-7b
        model_id: mistralai/Mistral-7B-Instruct-v0.3
        quantization: q4f16_1
        max_batch_size: 64
        tensor_parallel_shards: 1

    optimization:
      enable_speculative_decoding: true
      kv_cache_page_size: 16
      prefill_chunk_size: 2048
      max_num_sequences: 256

    routing:
      strategy: least_loaded
      health_check_interval: 10s
      timeout: 120s
