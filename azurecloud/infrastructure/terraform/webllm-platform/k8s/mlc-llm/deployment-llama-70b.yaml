apiVersion: apps/v1
kind: Deployment
metadata:
  name: mlc-llm-llama-3-1-70b
  namespace: mlc-llm
  labels:
    app: mlc-llm
    model: llama-3-1-70b
    tier: inference
spec:
  replicas: 2
  selector:
    matchLabels:
      app: mlc-llm
      model: llama-3-1-70b
  template:
    metadata:
      labels:
        app: mlc-llm
        model: llama-3-1-70b
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9090"
    spec:
      nodeSelector:
        gpu-type: a100
        agentpool: gpua100
      tolerations:
        - key: "nvidia.com/gpu"
          operator: "Equal"
          value: "true"
          effect: "NoSchedule"
        - key: "sku"
          operator: "Equal"
          value: "gpu"
          effect: "NoSchedule"
      containers:
        - name: mlc-llm
          image: webllmplatformacr.azurecr.io/mlc-llm:latest
          imagePullPolicy: Always
          ports:
            - containerPort: 8080
              name: http
            - containerPort: 9090
              name: metrics
          env:
            - name: MODEL_NAME
              value: "llama-3-1-70b"
            - name: MODEL_ID
              value: "meta-llama/Llama-3.1-70B-Instruct"
            - name: QUANTIZATION
              value: "q4f16_1"
            - name: MAX_BATCH_SIZE
              value: "32"
            - name: TENSOR_PARALLEL_SHARDS
              value: "4"
            - name: CUDA_VISIBLE_DEVICES
              value: "0,1,2,3"
          resources:
            limits:
              nvidia.com/gpu: 4
              memory: "160Gi"
            requests:
              nvidia.com/gpu: 4
              memory: "160Gi"
              cpu: "8"
          volumeMounts:
            - name: config
              mountPath: /app/config
            - name: model-cache
              mountPath: /models
            - name: shm
              mountPath: /dev/shm
          livenessProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 300
            periodSeconds: 30
            timeoutSeconds: 10
          readinessProbe:
            httpGet:
              path: /ready
              port: 8080
            initialDelaySeconds: 60
            periodSeconds: 10
            timeoutSeconds: 5
      volumes:
        - name: config
          configMap:
            name: mlc-llm-config
        - name: model-cache
          emptyDir:
            sizeLimit: 200Gi
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: 16Gi
      terminationGracePeriodSeconds: 60
---
apiVersion: v1
kind: Service
metadata:
  name: mlc-llm-llama-3-1-70b
  namespace: mlc-llm
  labels:
    app: mlc-llm
    model: llama-3-1-70b
spec:
  selector:
    app: mlc-llm
    model: llama-3-1-70b
  ports:
    - name: http
      port: 80
      targetPort: 8080
    - name: metrics
      port: 9090
      targetPort: 9090
  type: ClusterIP
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: mlc-llm-llama-3-1-70b-hpa
  namespace: mlc-llm
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: mlc-llm-llama-3-1-70b
  minReplicas: 2
  maxReplicas: 6
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
    - type: Pods
      pods:
        metric:
          name: mlc_llm_requests_in_queue
        target:
          type: AverageValue
          averageValue: "10"
