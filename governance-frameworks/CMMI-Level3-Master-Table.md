# CMMI Level 3 (Defined) — Single Master Table

> **Auditor-Ready | AI / GenAI / ML / RAG / Robotics**
>
> At Level 3, every project follows the same defined process, uses approved assets, manages risk formally, and is governed through measurable controls.

---

## Master Control Table

| # | Process Area | Purpose (Why) | Standard Process (How) | Key Activities | Mandatory Artifacts / Evidence |
|---|--------------|---------------|------------------------|----------------|-------------------------------|
| 1 | **Requirements Development & Management** | Ensure clear, controlled requirements | Elicit → Analyze → Approve → Baseline → Change Control | Stakeholder interviews, requirement analysis, traceability, change handling | Requirement Spec, RTM, Change Requests, Baseline Approval |
| 2 | **Estimation** | Predict effort, cost, schedule | Scope → WBS → Effort → Cost/Schedule → Risk Adjustment → Approval | Historical data use, complexity analysis, risk buffers | Estimation Sheet, Assumptions Log, Approval Record |
| 3 | **Project Planning** | Create executable project plan | Inputs → Schedule → Resource → Risk → Quality → Baseline | Planning workshops, RACI, milestone definition | Project Management Plan, Schedule, Risk Plan |
| 4 | **Process Asset Development** | Standardize how organization works | Identify → Define → Template → Tailor → Approve → Publish | SOP creation, template standardization | SOPs, Templates, Tailoring Guidelines, Process Library |
| 5 | **Implementation Infrastructure** | Enable consistent execution | Tool Selection → Access → Configuration → Usage → Support | Environment setup, access control | Tool Inventory, Access Logs, Setup Guides |
| 6 | **Technical Solution Implementation** | Build solution correctly | Design → Build → Review → Unit Test | Architecture design, component development | Architecture Doc, Design Spec, Code/Model, Unit Test Results |
| 7 | **Product Integration** | Ensure components work together | Integration Plan → Interface Check → Build → Fix | Interface validation, integration testing | Integration Plan, ICD, Integration Test Report |
| 8 | **Configuration Management** | Control versions and changes | Identify CI → Version → Baseline → Change → Audit | Versioning, change tracking | CM Plan, Change Log, Baseline Records |
| 9 | **Risk & Opportunity Management** | Reduce threats, exploit opportunities | Identify → Analyze → Mitigate → Monitor → Close | Risk workshops, mitigation tracking | Risk Register, Mitigation Plans, Review Logs |
| 10 | **Peer Reviews** | Detect defects early | Select → Prepare → Review → Log → Fix → Close | Peer inspections, defect analysis | Review Checklist, Defect Log, Review Summary |
| 11 | **Verification & Validation** | Ensure product meets specs & user needs | Verify vs Req → Validate vs Use Case | Test planning, execution, acceptance | Test Plan, Test Cases, Test Results, Acceptance Sign-off |
| 12 | **Monitoring, Control & Governance** | Keep project under control | Track → Analyze → Correct → Govern | KPI tracking, management reviews | Status Reports, KPI Dashboard, Audit Findings, CAPA |

---

## CMMI Level 3 for AI Systems

### What "CMMI Level 3 for AI" Means

AI is no longer experimental — it is a governed organizational capability.

| Level | AI Reality |
|-------|------------|
| Level 1 – Initial | Notebook-driven, POCs, tribal knowledge |
| Level 2 – Managed | Project-level AI processes |
| Level 3 – Defined | Enterprise AI standards & governance |
| Level 4 – Quantitative | AI performance statistically managed |
| Level 5 – Optimizing | Continuous AI optimization & innovation |

### Core Characteristics at Level 3 (AI View)

| Area | Level 3 Requirement |
|------|---------------------|
| AI Strategy | Documented AI vision & roadmap |
| Governance | AI policies & approval boards |
| Lifecycle | Standard AI/ML/GenAI lifecycle |
| Risk | Formal AI risk assessment |
| Data | Enterprise data governance |
| Models | Documented design & assumptions |
| GenAI | Prompt, RAG, and model governance |
| Security | AI-specific security controls |
| Compliance | Traceability & audit readiness |
| People | Defined AI roles & skills |

---

## Process Areas Mapped to AI

### Organizational Process Definition (OPD – AI)

| Requirement | AI Implementation |
|-------------|-------------------|
| Standard processes | AI SDLC / MLOps / LLMOps |
| Templates | Model cards, data sheets |
| Guidelines | GenAI usage standards |
| Tailoring | Project-specific adaptation |

### Organizational Training (OT – AI)

| Requirement | AI Example |
|-------------|------------|
| Skill plans | ML, GenAI, Responsible AI |
| Role-based training | DS, MLE, Product, Legal |
| Competency tracking | Certification & audits |

### Risk Management (RSKM – AI)

| AI Risk Type | Mandatory Control |
|--------------|-------------------|
| Bias | Bias testing & mitigation |
| Hallucination | Output validation |
| Data leakage | RAG guardrails |
| Model drift | Monitoring |
| Legal risk | Compliance checks |

### Decision Analysis & Resolution (DAR – AI)

| Requirement | AI Example |
|-------------|------------|
| Formal evaluation | Build vs buy LLM |
| Criteria | Cost, risk, explainability |
| Evidence | Documented decisions |

---

## GenAI / LLM / RAG — Level 3 Expectations

| GenAI Area | Level 3 Control |
|------------|-----------------|
| Prompt engineering | Approved templates |
| Prompt changes | Change management |
| RAG data | Approved sources only |
| Vector DB | Access-controlled |
| LLM selection | Formal DAR process |
| Fine-tuning | Dataset approval |
| Output | Guardrails & moderation |

---

## Mandatory AI Artifacts at CMMI Level 3

| Artifact | Purpose |
|----------|---------|
| AI Strategy Document | Direction & scope |
| AI Lifecycle SOP | Standard execution |
| AI Use-Case Intake Form | Controlled onboarding |
| AI Risk Register | Risk traceability |
| Data Governance Policy | Data quality & privacy |
| Model Documentation | Explainability |
| GenAI Prompt Policy | Prompt safety |
| RAG Architecture Spec | Data control |
| Human-in-the-Loop SOP | Oversight |
| AI Incident SOP | Failure handling |

---

## Process Flow

```
Requirements
     ↓
Estimation
     ↓
Project Planning
     ↓
Technical Solution
     ↓
Integration
     ↓
Verification & Validation
     ↓
Delivery

Supporting Processes (Continuous):
├── Process Assets
├── Infrastructure
├── Configuration
├── Risk
├── Peer Reviews
└── Monitoring & Governance
```

---

## What Auditors Check

| Area | Evidence Required |
|------|-------------------|
| Consistency | Same AI process everywhere |
| Documentation | SOPs, templates |
| Training | AI skill records |
| Risk handling | Mitigation actions |
| Traceability | From data → decision |
| Enforcement | Not optional |

### Pass Factors
- Same process across projects
- Documented tailoring
- Formal risk & decisions
- Training evidence
- Management enforcement

### Fail Factors
- Project-specific lifecycles
- Optional processes
- No training proof
- No decision records
- No QA audits

---

## Typical Gaps Preventing Level 3

- AI POCs without governance
- No AI intake or approval process
- GenAI usage without policy
- No bias or hallucination testing
- Data sources undocumented
- No AI risk owner

---

## Executive Summary

> **CMMI Level 3 for AI means AI is no longer a science experiment — it is a repeatable, auditable, enterprise capability.**

---

## Document Control

| Field | Value |
|-------|-------|
| Version | 1.0 |
| Classification | Internal / Audit-Ready |
| Applicable To | AI, GenAI, ML, RAG, Robotics, LLM |
| Framework Reference | CMMI V2.0 Level 3 |
